â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘   ğŸŒ JEEBS LEARNING EXPANSION - EXPLORE THE WEB! ğŸŒ        â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


WHAT YOU ASKED FOR:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"Widen the scope of Jeebs learning. When JeebsAI is going to
random websites follow links. Go exploring. Go to web pages
Jeebs has never found before. Never go to the same site."


WHAT I IMPLEMENTED:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… GLOBAL DOMAIN TRACKING
   â€¢ Tracks all domains Jeebs has visited across all crawls
   â€¢ Reads from database on startup
   â€¢ Never revisits the same domain twice
   â€¢ Ensures continuous exploration of new sites

âœ… INTELLIGENT LINK FOLLOWING
   â€¢ Prioritizes links to NEW domains (not yet visited)
   â€¢ Explores within new domains up to depth limit
   â€¢ Strongly filters out links back to visited domains
   â€¢ Encourages breadth of knowledge over depth in one site

âœ… DOMAIN DIVERSITY
   â€¢ Max 8 pages per domain (prevents getting stuck)
   â€¢ Spreads crawling across multiple sites
   â€¢ Balances exploration of new domains vs exploring within them
   â€¢ Tracks pages per domain to enforce limits

âœ… EXPANDED CRAWL SOURCES
   â€¢ Increased from 8 to 30 diverse websites
   â€¢ Science & research (NASA, ScienceDaily, Nature, etc)
   â€¢ Tech & AI (GitHub Trending, OpenAI, IBM Quantum, etc)
   â€¢ News (BBC, Guardian, The Economist, Wired, etc)
   â€¢ Developer resources (MDN, Rust, Python, FreeCodeCamp, etc)
   â€¢ Wikipedia random pages and portals
   â€¢ Academic (arXiv, PubMed, etc)

âœ… INCREASED EXPLORATION DEPTH
   â€¢ Doubled exploration from 25 â†’ 50 max pages per crawl
   â€¢ Crawls deeper following new domains
   â€¢ Maximizes unique content discovery
   â€¢ Follows links more aggressively


HOW IT WORKS - THE FLOW:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. Training cycle starts, picks random website from candidates
2. Loads list of previously crawled domains from database
3. Starts exploring from the selected website
4. Extracts links from each page
5. For each link:
   - Check if domain already visited
   - If NEW domain â†’ prioritize it! Add to queue immediately
   - If old domain â†’ only follow if less than limit reached
6. Visit page, store knowledge in brain
7. Mark domain as visited in database (never visit again)
8. Continue until 50 pages crawled or no more new domains
9. Next training cycle picks a different source
10. The database ensures NO DOMAIN IS EVER REVISITED


DATABASE PERSISTENCE:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Previously crawled domains are stored in jeebs_store table:
  key: "crawled_domain:example.com"
  value: timestamp of when crawled
  created_at: timestamp

This persists across:
  âœ… Server restarts
  âœ… Training cycles
  âœ… Admin crawls
  âœ… Random crawls


EXAMPLE CRAWL SESSION:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Training starts: Picks https://arxiv.org/

Visited domains: []
Queue: [(arxiv.org, depth 0)]

Page 1: arxiv.org/about â†’ finds links to:
  - nature.com (NEW!) â†’ priority add
  - arxiv.org/help (same domain)
  - github.com/pytorch (NEW!) â†’ priority add

Visited domains: [arxiv.org]
Queue: [(nature.com, 1), (github.com, 1), (arxiv.org, 1)]

Page 2: nature.com/articles â†’ finds links:
  - nature.com/issues (same domain, count < 8)
  - research.facebook.com (NEW!) â†’ priority add
  - science.org (NEW!) â†’ priority add

Visited domains: [arxiv.org, nature.com]
Queue: [(github.com, 1), (research.facebook.com, 1), (science.org, 1), (nature.com, 1)]

... continues exploring new domains until:
  â€¢ 50 pages crawled, OR
  â€¢ No more new domains to explore

Result: Jeebs learned from 15-20 different domains in ONE crawl!


BENEFITS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… DIVERSE KNOWLEDGE
   â€¢ Each crawl explores completely new territory
   â€¢ No wasted time revisiting old sites
   â€¢ Exponential growth in knowledge coverage

âœ… BREADTH OVER DEPTH
   â€¢ Learns from many sources, not just deep crawls
   â€¢ Cross-domain knowledge synthesis
   â€¢ Better pattern recognition from diverse data

âœ… CONTINUOUS EXPLORATION
   â€¢ Next training cycle picks from 30+ sources
   â€¢ Smart filtering ensures new domains each time
   â€¢ Database persistence prevents loops

âœ… EFFICIENT LEARNING
   â€¢ Follows link chains to related new sites
   â€¢ Discovers emerging domains through links
   â€¢ Learns in context (related pages lead to related sites)


CONFIGURATION OPTIONS (if needed):
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

These can be tuned in crawl_and_store function:
  â€¢ MAX_PAGES: 50 (max pages per crawl session)
  â€¢ MAX_PER_DOMAIN: 8 (max pages from single domain)
  â€¢ Depth limits: 1-3 (how deep to follow link chains)


â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… JEEBS WILL NOW EXPLORE THE WEB AND NEVER REVISIT!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
